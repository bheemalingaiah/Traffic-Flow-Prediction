# -*- coding: utf-8 -*-
"""traffic -STD-Loess-work6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FB6PO-Q-Fnl7OfcCd8rncTN9t0s5SA_4
"""

!pip install numpy pandas statsmodels tensorflow scipy

import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import STL
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# Sample time series data (replace with your dataset)
# Assuming `data` is a pandas DataFrame with a 'Date' column and 'Value' column
data = pd.DataFrame({
    'Date': pd.date_range(start='2020-01-01', periods=100, freq='D'),
    'Value': np.sin(np.linspace(0, 20, 100)) + np.random.normal(0, 0.2, 100)
})

# Set the 'Date' column as the index
data.set_index('Date', inplace=True)

# Step 1: Seasonal-Trend Decomposition using LOESS
stl = STL(data['Value'], seasonal=13, robust=True)
result = stl.fit()

# Extract the components
trend = result.trend
seasonal = result.seasonal
residual = result.resid

# Step 2: Normalization
scaler_trend = MinMaxScaler()
scaler_seasonal = MinMaxScaler()
scaler_residual = MinMaxScaler()

trend_normalized = scaler_trend.fit_transform(trend.values.reshape(-1, 1)).flatten()
seasonal_normalized = scaler_seasonal.fit_transform(seasonal.values.reshape(-1, 1)).flatten()
residual_normalized = scaler_residual.fit_transform(residual.values.reshape(-1, 1)).flatten()

# Add components to the DataFrame
data['Trend'] = trend
data['Seasonality'] = seasonal
data['Residual'] = residual
data['Trend_Normalized'] = trend_normalized
data['Seasonality_Normalized'] = seasonal_normalized
data['Residual_Normalized'] = residual_normalized

# Step 3: Visualization (Optional)
plt.figure(figsize=(8, 4))

# Original Time Series
plt.subplot(4, 1, 1)
plt.plot(data.index, data['Value'], label='Original', color='blue')
plt.title('Original Time Series')
#plt.legend()

# Trend Component
plt.subplot(4, 1, 2)
plt.plot(data.index, data['Trend'], label='Trend', color='orange')
plt.title('Trend Component')
#plt.legend()

# Seasonality Component
plt.subplot(4, 1, 3)
plt.plot(data.index, data['Seasonality'], label='Seasonality', color='green')
plt.title('Seasonality Component')
#plt.legend()

# Residual Component
plt.subplot(4, 1, 4)
plt.plot(data.index, data['Residual'], label='Residual', color='red')
plt.title('Residual Component')
#plt.legend()

plt.tight_layout()
plt.show()

import statsmodels.api as sm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Example time series data
time = np.arange(1, 101)
data = 10 + 2 * np.sin(2 * np.pi * time / 12) + np.random.normal(0, 0.5, len(time))

# Seasonal-Trend decomposition using LOESS
df = pd.DataFrame({'time': time, 'value': data})
decomposition = sm.tsa.seasonal_decompose(df['value'], model='additive', period=12)

# Extract components
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

# Plot LOESS decomposition components
plt.figure(figsize=(14, 10))

plt.subplot(4, 1, 1)
plt.plot(df['time'], df['value'], label='Original Data')
plt.title('Original Time Series')
plt.legend()

plt.subplot(4, 1, 2)
plt.plot(df['time'], trend, color='orange', label='Trend')
plt.title('Trend Component')
plt.legend()

plt.subplot(4, 1, 3)
plt.plot(df['time'], seasonal, color='green', label='Seasonal')
plt.title('Seasonal Component')
plt.legend()

plt.subplot(4, 1, 4)
plt.plot(df['time'], residual, color='red', label='Residual')
plt.title('Residual Component')
plt.legend()

plt.tight_layout()
plt.show()

# Plot correlation graph between trend and seasonal components
valid_indices = ~np.isnan(trend)  # Remove NaNs from trend
plt.figure(figsize=(8, 6))
plt.scatter(trend[valid_indices], seasonal[valid_indices], alpha=0.7, color='purple')
plt.title('Correlation between Trend and Seasonal Components')
plt.xlabel('Trend')
plt.ylabel('Seasonal')
plt.grid()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Example data: Actual traffic flow and predicted quantiles
np.random.seed(42)
time = np.arange(1, 101)  # Example time steps
actual_values = np.random.poisson(20, len(time))  # Example actual traffic flow data

# Predicted quantiles: 10th, 50th (median), and 90th percentiles
predicted_quantiles = {
    "10th": actual_values - np.random.uniform(3, 5, len(time)),
    "50th": actual_values + np.random.uniform(-2, 2, len(time)),
    "90th": actual_values + np.random.uniform(3, 5, len(time)),
}

# Create a DataFrame for visualization
df = pd.DataFrame({
    "Time": time,
    "Actual": actual_values,
    "10th Quantile": predicted_quantiles["10th"],
    "50th Quantile": predicted_quantiles["50th"],
    "90th Quantile": predicted_quantiles["90th"],
})

# Visualization
plt.figure(figsize=(7, 4))
plt.plot(df["Time"], df["Actual"], label="Actual Traffic Flow", color="blue", marker='o')
plt.plot(df["Time"], df["50th Quantile"], label="50th Quantile (Median Prediction)", color="green")
plt.fill_between(
    df["Time"],
    df["10th Quantile"],
    df["90th Quantile"],
    color="lightgray",
    label="10th-90th Quantile Range"
)
plt.title("Quantile Predictions vs. Actual Traffic Flow")
plt.xlabel("Time")
plt.ylabel("Traffic Flow")
#plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

# Numerical comparison: Mean Absolute Error (MAE) for each quantile
mae_50th = np.mean(np.abs(df["50th Quantile"] - df["Actual"]))
print(f"MAE for 50th Quantile (Median Prediction): {mae_50th:.2f}")

import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import STL
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# Load the dataset
file_path = '/content/sample_data/Traffic (1).csv'
data = pd.read_csv(file_path)

# Convert 'Date' column to datetime
data['Time'] = pd.to_datetime(data['Time'])
data = data.set_index('Time')  # Set 'Date' as index

# Infer the frequency
# This infers the most likely frequency from the data
inferred_freq = pd.infer_freq(data.index)

# If a frequency is successfully inferred, resample the data
if inferred_freq is not None:
    data = data.asfreq(inferred_freq)
else:
    print("Warning: Could not infer frequency. Data not resampled.")
    # You might need to manually specify the frequency using 'D', 'W', 'M', etc.

# Select the 'Total' column for decomposition
target_column = 'Total'

# Step 1: Seasonal-Trend Decomposition using LOESS (STD)
period = 12 # Assuming weekly seasonality for traffic data
stl = STL(data[target_column], seasonal=13, period=period, robust=True) # Add the period parameter
result = stl.fit()

#result = stl.fit()

# Extract the components
trend = result.trend
seasonal = result.seasonal
residual = result.resid

# Step 2: Normalization
scaler_trend = MinMaxScaler()
scaler_seasonal = MinMaxScaler()
scaler_residual = MinMaxScaler()

trend_normalized = scaler_trend.fit_transform(trend.values.reshape(-1, 1)).flatten()
seasonal_normalized = scaler_seasonal.fit_transform(seasonal.values.reshape(-1, 1)).flatten()
residual_normalized = scaler_residual.fit_transform(residual.values.reshape(-1, 1)).flatten()

# Add components to the DataFrame
data['Trend'] = trend
data['Seasonality'] = seasonal
data['Residual'] = residual
data['Trend_Normalized'] = trend_normalized
data['Seasonality_Normalized'] = seasonal_normalized
data['Residual_Normalized'] = residual_normalized

# Step 3: Save Results to a CSV
output_file_name = "decomposed_traffic_data.csv"
data.to_csv(output_file_name)
print(f"Decomposed data saved to {output_file_name}")

# Step 4: Visualization
plt.figure(figsize=(12, 8))

# Original Time Series
plt.subplot(4, 1, 1)
plt.plot(data.index, data[target_column], label='Original', color='blue')
plt.title('Original Time Series')
plt.legend()

# Trend Component
plt.subplot(4, 1, 2)
plt.plot(data.index, data['Trend'], label='Trend', color='orange')
plt.title('Trend Component')
plt.legend()

# Seasonality Component
plt.subplot(4, 1, 3)
plt.plot(data.index, data['Seasonality'], label='Seasonality', color='green')
plt.title('Seasonality Component')
plt.legend()

# Residual Component
plt.subplot(4, 1, 4)
plt.plot(data.index, data['Residual'], label='Residual', color='red')
plt.title('Residual Component')
plt.legend()

plt.tight_layout()
plt.show()

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
import numpy as np

# Define a function to build QR-DQN with variable hyperparameters
def build_qr_dqn(input_shape, n_quantiles, quantile_levels, units=(128, 64), dropout_rate=0.2):
    """
    Build Quantile Regression Deep Queue Network (QR-DQN) with flexible parameters.
    :param input_shape: Shape of the input features
    :param n_quantiles: Number of quantiles to predict
    :param quantile_levels: List of quantile levels
    :param units: Tuple defining the number of units in each hidden layer
    :param dropout_rate: Dropout rate for regularization
    """
    inputs = Input(shape=input_shape)
    x = inputs

    # Hidden Layers
    for unit in units:
        x = Dense(unit, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(dropout_rate)(x)

    # Output Layer
    outputs = [Dense(1, name=f'quantile_{q}')(x) for q in quantile_levels]

    model = Model(inputs, outputs)
    return model

# Define a function to train a QR-DQN
def train_qr_dqn(model, X_train, y_train_quantiles, X_test, y_test_quantiles, learning_rate=0.001, epochs=50, batch_size=32):
    """
    Train a QR-DQN model.
    :param model: QR-DQN model
    :param X_train: Training features
    :param y_train_quantiles: Training targets for each quantile
    :param X_test: Testing features
    :param y_test_quantiles: Testing targets for each quantile
    :param learning_rate: Learning rate for the optimizer
    :param epochs: Number of training epochs
    :param batch_size: Batch size for training
    """
    losses = {f'quantile_{q}': lambda y, y_pred: quantile_loss(q, y, y_pred) for q in quantile_levels}
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=losses)

    history = model.fit(
        X_train, y_train_quantiles,
        validation_data=(X_test, y_test_quantiles),
        epochs=epochs,
        batch_size=batch_size,
        verbose=0  # Set verbose=1 to view training progress
    )
    return model, history

# Ensemble of QR-DQNs
ensemble_size = 5  # Number of models in the ensemble
ensemble_models = []
predictions_list = []

# Train multiple QR-DQNs with different hyperparameters
for i in range(ensemble_size):
    print(f"Training model {i + 1}/{ensemble_size}...")

    # Create model with different hyperparameters
    units = (128 + i * 32, 64 + i * 16)  # Vary the number of units in each layer
    dropout_rate = 0.2 + i * 0.05        # Vary dropout rate
    model = build_qr_dqn(input_shape=input_shape, n_quantiles=n_quantiles, quantile_levels=quantile_levels,
                         units=units, dropout_rate=dropout_rate)

    # Train the model
    trained_model, _ = train_qr_dqn(model, X_train, y_train_quantiles, X_test, y_test_quantiles,
                                    learning_rate=0.001, epochs=50, batch_size=32)

    # Store the trained model
    ensemble_models.append(trained_model)

    # Make predictions and store them
    predictions = trained_model.predict(X_test)
    predictions_list.append(predictions)

# Combine predictions from all ensemble models (simple averaging)
ensemble_predictions = np.mean(predictions_list, axis=0)

# Display Predictions for Each Quantile
for i, q in enumerate(quantile_levels):
    print(f"Ensemble Predictions for {q * 100}th percentile: {ensemble_predictions[i][:5]}")

import tensorflow as tf
import numpy as np
from scipy.stats import norm
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Continuous Ranked Probability Score (CRPS)
def crps(y_true, y_pred_quantiles, quantile_levels):
    """
    Compute CRPS for probabilistic forecasts.
    :param y_true: True target values (N,)
    :param y_pred_quantiles: Predicted quantile values (N, Q)
    :param quantile_levels: Quantile levels (Q,)
    """
    N, Q = y_pred_quantiles.shape
    F = np.zeros((N, Q))
    for i, q in enumerate(quantile_levels):
        F[:, i] = (y_pred_quantiles[:, i] >= y_true).astype(float)

    cdf = np.cumsum(F, axis=1) / Q
    crps_scores = np.trapz((cdf - np.heaviside(y_true[:, None] - y_pred_quantiles, 0))**2, quantile_levels, axis=1)
    return np.mean(crps_scores)

# Ensemble CRPS (eCRPS)
def ensemble_crps(y_true, ensemble_predictions, quantile_levels):
    """
    Compute eCRPS by averaging CRPS across ensemble models.
    :param y_true: True target values (N,)
    :param ensemble_predictions: List of ensemble predictions, each (N, Q)
    :param quantile_levels: Quantile levels (Q,)
    """
    crps_scores = []
    for predictions in ensemble_predictions:
        crps_scores.append(crps(y_true, predictions, quantile_levels))
    return np.mean(crps_scores)

# Postprocessing: Aggregate Quantiles
def construct_prediction_intervals(predictions, quantile_levels, interval=[0.1, 0.9]):
    """
    Construct prediction intervals (e.g., 80% or 95% confidence intervals).
    :param predictions: Predicted quantile values (N, Q)
    :param quantile_levels: Quantile levels (Q,)
    :param interval: Confidence interval bounds (e.g., [0.1, 0.9])
    """
    lower_bound = predictions[:, quantile_levels.index(interval[0])]
    upper_bound = predictions[:, quantile_levels.index(interval[1])]
    return lower_bound, upper_bound

# Evaluation Metrics
def evaluate_model(y_true, point_forecast, quantile_forecasts, quantile_levels, ensemble_predictions=None):
    """
    Evaluate the model using various metrics.
    :param y_true: True target values (N,)
    :param point_forecast: Point forecast values (N,)
    :param quantile_forecasts: Predicted quantile values (N, Q)
    :param quantile_levels: Quantile levels (Q,)
    :param ensemble_predictions: Optional list of ensemble predictions, each (N, Q)
    """
    metrics = {}
    metrics['MAE'] = mean_absolute_error(y_true, point_forecast)
    metrics['MSE'] = mean_squared_error(y_true, point_forecast)

    # CRPS for quantile forecasts
    # Reshape quantile_forecasts to (N, Q)
    quantile_forecasts_reshaped = np.concatenate([np.array(model_predictions).squeeze() for model_predictions in quantile_forecasts], axis=1)
    metrics['CRPS'] = crps(y_true, quantile_forecasts_reshaped, quantile_levels)


    # eCRPS for ensemble models
    if ensemble_predictions is not None:
        metrics['eCRPS'] = ensemble_crps(y_true, ensemble_predictions, quantile_levels)

    return metrics

# Recompose Final Forecast (Trend + Seasonality)
def recompose_forecast(trend, seasonality, residual_forecast):
    """
    Recompose final forecast by adding back trend and seasonality.
    :param trend: Trend component (N,)
    :param seasonality: Seasonality component (N,)
    :param residual_forecast: Forecasted residual values (N,)
    """
    return trend + seasonality + residual_forecast

# Example Workflow
print("Evaluating Ensemble Model...")

# Calculate the point forecast as the median (50th percentile)
# Updated line to access the correct dimension for the median (50th percentile)
point_forecast = ensemble_predictions[1, :, 0]  # Assuming 0.5 is the second quantile (50th percentile)

# Evaluate the model
metrics = evaluate_model(y_test, point_forecast, predictions_list, quantile_levels)
print(f"Evaluation Metrics: {metrics}")


# Construct 80% Prediction Interval
lower_bound, upper_bound = construct_prediction_intervals(ensemble_predictions, quantile_levels, interval=[0.1, 0.9])
print(f"80% Prediction Interval:\nLower Bound: {lower_bound[:5]}\nUpper Bound: {upper_bound[:5]}")

# Recompose Final Forecast
final_forecast = recompose_forecast(data['Trend'][-len(y_test):], data['Seasonality'][-len(y_test):], point_forecast)
print(f"Final Forecast (First 5): {final_forecast[:5]}")

import matplotlib.pyplot as plt
import numpy as np

# Data
methods = [
    "LSTM",
    "GRU",
    "GCN",
    "ST-GCN",
    "Transformer-Based Models",
    "Proposed QR-DQN with STD and eCRPS"
]
accuracy = [86.25, 87.65, 89.69, 91.05, 91.09, 95.18]
rmse = [7.8, 7.6, 7.2, 6.9, 6.5, 6.1]
mae = [5.2, 5.1, 4.8, 4.5, 4.3, 4.1]

# Colors for bars
colors = ['lightcoral', 'lightgoldenrodyellow', 'pink', 'lightgreen', 'lightblue', 'plum']

# Plot for Accuracy Comparison
fig1, ax1 = plt.subplots(figsize=(10, 6))
bar1 = ax1.bar(methods, accuracy, color=colors)
ax1.set_xlabel("Methods", fontsize=13,fontweight="bold")
ax1.set_ylabel("Accuracy (%)", fontsize=13,fontweight="bold")
ax1.set_title("Accuracy Comparison of Traffic Prediction Methods", fontsize=14,fontweight="bold")
ax1.set_xticks(np.arange(len(methods)))
ax1.set_xticklabels(methods, rotation=20, ha='right', fontsize=10,fontweight="bold")
ax1.set_ylim(60, 100)  # Adjusting the y-axis range

# Annotate bars with values
for bar in bar1:
    height = bar.get_height()
    ax1.text(
        bar.get_x() + bar.get_width() / 2.0,
        height,
        f"{height:.2f}",
        ha="center",
        va="bottom",
        fontsize=10,
        fontweight="bold"
    )

plt.tight_layout()
plt.show()

# Plot for RMSE and MAE Comparison
x = np.arange(len(methods))
width = 0.35

fig2, ax2 = plt.subplots(figsize=(10, 6))
bar_rmse = ax2.bar(x - width/2, rmse, width, label="RMSE", color='lightcoral')
bar_mae = ax2.bar(x + width/2, mae, width, label="MAE", color='lightgreen')

ax2.set_xlabel("Methods", fontsize=13,fontweight="bold")
ax2.set_ylabel("Error Values", fontsize=13,fontweight="bold")
ax2.set_title("RMSE and MAE Comparison of Traffic Prediction Methods", fontsize=14,fontweight="bold")
ax2.set_xticks(x)
ax2.set_xticklabels(methods, rotation=20, ha='right', fontsize=10,fontweight="bold")
ax2.set_ylim(0, 9)
ax2.legend()

# Annotate bars with values
def add_values(bars):
    for bar in bars:
        height = bar.get_height()
        ax2.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.2f}",
            ha="center",
            va="bottom",
            fontsize=10,
            fontweight="bold"
        )


add_values(bar_rmse)
add_values(bar_mae)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data
models = [
    "Full Model (QR-DQN + STD + eCRPS)",
    "QR-DQN + STD",
    "QR-DQN + eCRPS",
    "STD + eCRPS",
    "QR-DQN Only",
    "STD Only",
    "eCRPS Only"
]
accuracy = [95.18, 91.06, 92.54, 89.98, 87.78, 85.64, 86.08]
mae = [4.0, 4.5, 4.3, 4.8, 5.0, 5.3, 5.1]
rmse = [6.2, 6.8, 6.5, 7.0, 7.4, 7.8, 7.6]

# X-axis positions
x = np.arange(len(models))
width = 0.25  # Bar width

# Create the bar graph
fig, ax = plt.subplots(figsize=(12, 5))

bar1 = ax.bar(x - width, accuracy, width, label="Accuracy (%)", color='skyblue')
bar2 = ax.bar(x, mae, width, label="MAE", color='lightcoral')
bar3 = ax.bar(x + width, rmse, width, label="RMSE", color='lightgreen')

# Add labels, title, and legend
ax.set_xlabel("Model Configurations", fontsize=12,fontweight='bold')
ax.set_ylabel("Metrics", fontsize=12,fontweight='bold')
ax.set_title("Performance Comparison of Model Configurations", fontsize=14,fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=20, ha='right', fontsize=10,fontweight='bold')
ax.legend()

# Annotate bars with values
def add_values(bars):
    for bar in bars:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.2f}",
            ha="center",
            va="bottom",
            fontsize=10,
            fontweight='bold'
        )

add_values(bar1)
add_values(bar2)
add_values(bar3)

# Adjust layout
plt.tight_layout()
plt.show()